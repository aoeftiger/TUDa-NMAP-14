{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38250724",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"text-align: center; vertical-align: middle;\">Numerical Methods of Accelerator Physics</h1>\n",
    "<h2 style=\"text-align: center; vertical-align: middle;\">Lecture Series by Dr. Adrian Oeftiger</h2>\n",
    "\n",
    "<h3 style=\"text-align: center; vertical-align: middle; margin-top: 1em; margin-bottom: 1em;\">Guest Lecture by Dr. Andrea Santamaria Garcia and Chenran Xu</h3>\n",
    "\n",
    "<div style=\"width: 45%; margin: auto; vertical-align: middle; \">\n",
    "<img src=\"./img/etit.png\" style=\"width: 40%; float: left; vertical-align: center\" /><img src=\"./img/KIT_logo.svg\" style=\"width: 38%; float: right; vertical-align: center;\" />\n",
    "</div>\n",
    "<div style=\"clear: both; vertical-align: middle;\"></div>\n",
    "\n",
    "<h3 style=\"text-align: center; vertical-align: middle; \">Part 14: 10.02.2023</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a8d74b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Run this notebook online!</h2>\n",
    "\n",
    "Interact and run this jupyter notebook online:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"text-align:center;\">\n",
    "1. via the public mybinder.org service: <br />\n",
    "\n",
    "<p style=\"text-align: center; margin-left, margin-right: auto; width: 100%;\">\n",
    "<a href=\"https://mybinder.org/v2/gh/aoeftiger/TUDa-NMAP-14/v1.0\"><img src=\"./img/binder_logo.svg\" /></a>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\" style=\"text-align:center;\">\n",
    "2. on the <a href=\"https://tu-jupyter-i.ca.hrz.tu-darmstadt.de/\">local TU Darmstadt jupyterhub $\\nearrow$</a> (using your TU ID)\n",
    "\n",
    "$\\implies$ make sure you installed all the required python packages (see the [README](./README.md))!\n",
    "</div>\n",
    "\n",
    "Finally, also find this lecture rendered [as HTML slides on github $\\nearrow$](https://aoeftiger.github.io/TUDa-NMAP-14/) along with the [source repository $\\nearrow$](https://github.com/aoeftiger/TUDa-NMAP-14)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57cc074",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Run this first!</h2>\n",
    "\n",
    "Imports and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e538eebc",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'botorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reload\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "File \u001b[0;32m~/gsi/documents/lectures/TUDa-NMAP-14/config.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, Union\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgpytorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'botorch'"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "from importlib import reload\n",
    "import time\n",
    "import config\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa5f101",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"color: #b51f2a\">Preparation</h2>\n",
    "\n",
    "- <a href=\"https://moodle.tu-darmstadt.de/mod/lti/view.php?id=1101645\">Introduction to machine learning applied to particle accelerators</a> by Dr. Andrea Santamaria Garcia\n",
    "- <a href=\"https://moodle.tu-darmstadt.de/mod/lti/view.php?id=1101645\">Machine learning activities at the accelerators of KIT</a> by Dr. Andrea Santamaria Garcia\n",
    "- <a href=\"https://moodle.tu-darmstadt.de/mod/lti/view.php?id=1101645\">Introduction to Bayesian optimization</a> by Chenran Xu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9cec0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"color: #b51f2a\">Today!</h2>\n",
    "\n",
    "In this tutorial notebook we will implement all the basic components of **Bayesian optimization (BO)** to find the global maximum of an unknown function, and see an example applied to accelerators.\n",
    "\n",
    "- <a href=\"http://localhost:8888/notebooks/lecture.ipynb#Part-I:-Bayesian-Optimization-Theory\">Part I: Bayesian Optimization Theory</a>\n",
    "- <a href=\"http://localhost:8888/notebooks/lecture.ipynb#Part-II:-Bayesian-Optimization-Algorithm-Implementation\">Part II: Bayesian Optimization Algorithm implementation</a>\n",
    "- <a href=\"http://localhost:8888/notebooks/lecture.ipynb#Part-III:-Bayesian-Optimization-for-Beam-Positioning-and-Focusing-in-a-Linac\">Part III: Bayesian Optimization for Beam Positioning and Focusing in a Linac</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54b239f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"color: #b51f2a\">Abbreviations used in this notebook</h2>\n",
    "\n",
    "- **BO**: Bayesian optimization\n",
    "- **GP**: Gaussian process\n",
    "\n",
    "<h2 style=\"color: #b51f2a\">Jargon</h2>\n",
    "\n",
    "These terms are used interchangeably:\n",
    "\n",
    "- __Objective, metric, target function__: $f(x)$ an unknown (black-box) function, for which the value is to be optimized (here: maximization)\n",
    "- __Observation, function evaluation, function query, data point__: $y=f(x)$, the value of function $f$ at a particular value of $x$\n",
    "- __Feature, tuning parameter, \"knob\", dimension, actuator__: $x_i = x_0, x_1, ...$, dimensions of your problem, correlated parameters the algorithm will vary\n",
    "- __Search space, bounds, optimization range__: a (continuous) parameter space where the input parameters are allowed to be varied in the optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c56d4e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: center; width:100%;\">\n",
    "    <h2>Part I: Bayesian Optimization Theory</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b032e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Bayes' Theorem</h2>\n",
    "\n",
    "Bayes' theorem is used for __statistical inference__, i.e. the process of using data analysis to infer properties of an underlying distribution of probability, or in other words, the process of drawing conclusions from data subject to random variations. The Bayes' theorem reads:\n",
    "    \n",
    "<img src=\"img/bayes.png\" style=\"width:50%; margin:auto;\"/>\n",
    "<p style=\"clear:both; font-size: small; text-align: center; margin-top:1em;\">image from <a href=\"https://www.freecodecamp.org/news/bayes-rule-explained/\">Bayes' Rule – Explained For Beginners</a></p>\n",
    "\n",
    "- $A, B$ are events, where $A$ can be interpreted as our hypothesis and $B$ as getting evidence\n",
    "- $P(A|B)$ is the <span style='color:#b51f2a'>**posterior probability**</span> of event $A$ happening given that event $B$ is observed, or the probability that the hypothesis is true given the evidence.\n",
    "- $P(B|A)$ is the <span style='color:#b51f2a'>**likelihood probability**</span> the conditional probablity of observing $B$ given $A$, or the probability of seeing the evidence if the hypothesis is true.\n",
    "- $P(A), P(B)$ are the <span style='color:#b51f2a'>**independent probabilities**</span> of observing $A$ and $B$, where:\n",
    "    - $P(A)$ is know as the <span style='color:#b51f2a'>**prior probability**</span>, or probability that a hypothesis is true before any evidence (observation) is present\n",
    "    - $P(B)$ as the <span style='color:#b51f2a'>**marginal probability**</span>, or probability of observing the evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51490f0c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Bayes' Theorem</h2>\n",
    "    \n",
    "Usually when we apply Bayesian optimization to a problem of our choice we make sure that when we query $f(x)$ we always get an observation $y$ back, so the probability of getting new samples is always 100%, i.e., $P(B) = 1$\n",
    "\n",
    "Thus the Bayes' theorem reads\n",
    "    $$P(A|B) \\propto P(B|A) P(A),$$\n",
    "    \n",
    "i.e. the __posterior probability__ is proportional to the __prior__ times the __likelihood__ probabilities.\n",
    "\n",
    "<h3>Why use it?</h3>\n",
    "\n",
    "Bayesian inference links the degree of belief in a hypothesis before and after accounting for evidence (observations), so it's ideal for building a probabilistic model and updating it sequentially as new data is gathered:\n",
    "\n",
    "<img src=\"img/bayes_2.png\" style=\"width:70%; margin:auto;\" />\n",
    "<p style=\"clear:both; font-size: small; text-align: center; margin-top:1em;\">image from <a href=\"https://www.gaussianwaves.com/2021/04/bayes-theorem/\">Bayes’ theorem</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d48e4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Bayesian Optimization</h2>\n",
    "\n",
    "__Bayesian optimization (BO)__ is a sequential algorithm to globally optimize an unknown function $f(x)$. In other words, it solves problems of the form\n",
    "\n",
    "$$\\text{max}_{x \\in N} f(x),$$\n",
    "\n",
    "where $N$ is a set of observations.\n",
    "\n",
    "The steps taken by a BO algorithm are the following:\n",
    "\n",
    "- Bayesian optimization treats $f(x)$ as a __random function__ and places a <span style='color:#b51f2a'>__prior__</span> over it. The prior might contain previous observations (informed prior) or no information at all. These probability distributions are drawn from a probabilistic model like <span style='color:#b51f2a'>__Gaussian Processes__</span> (more information <a href=\"http://localhost:8892/notebooks/lecture.ipynb#Gaussian-Process-(GP)\">here</a>).\n",
    "- We gather new observations (getting the value of $f(x)$ at particular $x$ points).\n",
    "- The <span style='color:#b51f2a'>__posterior__</span> distribution is obtained by applying Bayes' theorem.\n",
    "- The <span style='color:#b51f2a'>__posterior__</span> distribution is used to build an <span style='color:#b51f2a'>__acquisition function__</span> that determines the next query point (more information <a href=\"http://localhost:8892/notebooks/lecture.ipynb#Acquisition-function)\">here</a>)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef641d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Acquisition function</h2>\n",
    "\n",
    "The __acquisition function__ $\\alpha$ is:\n",
    "- Built on the GP posterior, \n",
    "- Controls the behavior of optimization by balancing exploration and exploitation to minimize the number of function queries (observations). \n",
    "- For the standard verison of BO, the next sample point is chosen at $\\mathrm{argmax}(\\alpha)$. \n",
    "- In this tutorial we will introduce two widely used acquisition functions: the expected improvement (EI) and the upper confidence bound (UCB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34551a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Gaussian Process (GP)</h2>\n",
    "<h3>Introduction</h3>\n",
    "\n",
    "A Gaussian Process (GP):\n",
    "- is a way to construct your <span style='color:#b51f2a'>__prior__</span> and <span style='color:#b51f2a'>__posterior__</span> distributions.\n",
    "- is a collection of random variables, where every linear combination of them is __normally distributed__.\n",
    "- can be used as a __probablistic model__ (surrogate model) of the objective function $f(x)$.\n",
    "    - one can consider it like a __function generator__, where all the functions drawn from the model will follow specific statistical properties.\n",
    "\n",
    "A GP can be fully described by it's mean $\\mu$ and covariance function $k(\\cdot,\\cdot)$\n",
    "    $$f(x) \\sim \\mathcal{GP}(\\mu(x), k(x,x'))$$\n",
    "    \n",
    "where $x$ and $x'$ are points in the input space.\n",
    "\n",
    "_Note: in GP regression, one usually understands $x'$ as the observed points, and $x$ as the continuous variable_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a04595",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Gaussian Process (GP)</h2>\n",
    "<h3>Definitions</h3>\n",
    "\n",
    "- <span style='color:#b51f2a'>__Kernel__</span> or <span style='color:#b51f2a'>__covariance function__</span> $k(\\cdot,\\cdot)$, is just a general name for a function $k$ of two arguments mapping a pair of inputs $x \\in \\mathcal{X}, x' \\in \\mathcal{X}$ into $\\mathbb{R}$\n",
    "    - is the basic building block of GPs\n",
    "    - encodes the assumptions about the function we wish to learn\n",
    "    - is a measure of how much to random variables (features) vary together (a measure of similarity)\n",
    "- <span style='color:#b51f2a'>__Covariance matrix__</span> : given a set of points $\\{x_i\\}$ we can compute the covariance matrix $K$ whose entries are $K_{ij}=k(x_i, x_j)$, where $k$ is the covariance function.\n",
    "- <span style='color:#b51f2a'>__Prior mean__ </span>$\\mu(x)$: prior belief on the averaged objective function values, usually set to a constant if the function behavior is unknown.\n",
    "\n",
    "- <span style='color:#b51f2a'>__Gaussian process regression (GPR)__</span>, also formerly known as _kriging_: a method to interpolate an unknown objective function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4618a2c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Gaussian Process (GP)</h2>\n",
    "<h3> Bonus: would an arbitrary function of input pairs $x$ and $x'$ be a valid covariance function?</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d9eeeb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Gaussian Process (GP)</h2>\n",
    "<h3> Bonus: would an arbitrary function of input pairs $x$ and $x'$ be a valid covariance function?</h3>\n",
    "\n",
    "The answer is no, as it has to be __positive semidefinite__. \n",
    "\n",
    "In this way the intersection of the covariance matrices for multiple features can have a global minima. More information and geometrical interpretation <a href=\"https://gowrishankar.info/blog/why-covariance-matrix-should-be-positive-semi-definite-tests-using-breast-cancer-dataset/\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f434cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Gaussian Process (GP)</h2>\n",
    "<h3>Hyperparameters</h3>\n",
    "\n",
    "The characteristics of GP, or its ability to approximate the unknown function are dependent both on __the choice of the covariance function__ and the __values of the hyperparamters__. The hyperparamters are usually either choosen manually based on the physics, or obtained from the maximum likelihood fit (log-likelihood fit, maximum a posteriori fit) during the optimization.\n",
    "\n",
    "- <span style='color:#b51f2a'>__Lengthscale__ $l$</span> : controls the scaling of different input dimensions, i.e. how fast the objective function is expected to change from observed points, how smooth the function is.\n",
    "    - small lengthscale = function values can change quickly\n",
    "    - large lengthscale = function changes slowly\n",
    "- <span style='color:#b51f2a'>__Signal variance__ $\\sigma^2$</span>: a scaling factor to be multiplied to the kernel function (explained next), it is essentially equivalent to normalizing/scaling the objective function.\n",
    "    - small signal variance = functions stay close to their mean value\n",
    "- <span style='color:#b51f2a'>__Noise variance__ $\\sigma_\\mathrm{n}^2$</span>: magnitude of the noise in the observed values, how much noise is expected to be present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d126df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Gaussian Process (GP)</h2>\n",
    "<h3>Covariance functions (kernels)</h3>\n",
    "\n",
    "Some of the commonly used kernels are listed below can be found <a href=\"https://en.wikipedia.org/wiki/Gaussian_process#Usual_covariance_functions\">here</a>. They can also be combined to build more complex kernels representing the underlying physics of the objective function.\n",
    "\n",
    "In this notebook we will use a scaled <span style='color:#b51f2a'>__radial basis function (RBF) kernel__</span>\n",
    "$$k_{\\mathrm{RBF}} (x,x') = \\exp\\left(-\\frac{d_{x,x'}^2}{2 l^2}\\right),$$\n",
    "\n",
    "where:\n",
    "- $l$ = lenghtscale\n",
    "- $ d_{x,x'} := ||x - x'||^2 $ is the Euclidean distance between the two points\n",
    "\n",
    "It is also known as squared exponential (SE). This resembles a normal Gaussian distribution. \n",
    "\n",
    "It is more or less _the default_ choice of kernel for GPs if one does not have a special assumption on the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875b5004",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: center; width:100%;\">\n",
    "    <h2>Part II: Bayesian Optimization Algorithm Implementation</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ccdbe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>BoTorch: Bayesian Optimization in PyTorch</h2>\n",
    "\n",
    "- <a href=\"https://botorch.org/\">BoTorch</a> is the leading Python library to implement Bayesian optimization.\n",
    "It is built on top of <a href=\"https://pytorch.org/\">PyTorch</a>, which is an optimized tensor library for deep learning using GPUs and CPUS.\n",
    "- A <span style='color:#b51f2a'> **tensor** </span> is an algebraic object that may map between different objects such as vectors, scalars, and even other tensors. It can be easily understood as a multidimensional matrix/array. \n",
    "    - These objects allow to easily carry out machine learning computations in problems with many features, weights, etc.\n",
    "    - In PyTorch, a <a href=\"https://pytorch.org/docs/stable/tensors.html#:~:text=A%20torch.,of%20a%20single%20data%20type.\">tensor</a> is a multi-dimensional matrix containing elements of a single data type.\n",
    "\n",
    "<img src=\"img/tensor_2.jpeg\" style=\"width:50%; margin:auto;\" />\n",
    "<p style=\"clear:both; font-size: small; text-align: center; margin-top:1em;\">image from <a href=\"https://towardsai.net/p/deep-learning/working-with-pytorch-tensors\">Working with PyTorch tensors</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6944e29f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>BoTorch: Bayesian Optimization in PyTorch</h2>\n",
    "\n",
    "- **Accelerated computing** = when we add extra hardware to accelerate computation, like GPUs (needed in deep machine learning).\n",
    "    - GPU: many \"not-so-intelligent\" cores that are parallelizable. They can carry out specific operations in a very efficient way, e.g. tensor cores perform very efficient sparse tensor multiplication.\n",
    "- **We will be working with torch tensors in this notebook!** instead of the usual numpy arrays. This means you could execute this code on a GPU if you have access to one with a simple command `torch.device(\"cuda\")`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9f9e2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Build an objective function and create some random observations</h2>\n",
    "\n",
    "Let's assume we want to find the global maximum of this function:\n",
    "\n",
    "$$f(x) = \\sin (2 \\pi x) + \\epsilon$$\n",
    "\n",
    "Let's start by setting a random seed for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb1f32",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "random_seed = 3\n",
    "rng = torch.random.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7108eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Create some observation points with noise</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376d3d09",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "xmin, xmax = 0, 1\n",
    "observations_tot = 10\n",
    "noise_level = 0.2  # add some white noise to observations\n",
    "observations_x = torch.rand(observations_tot, 1, generator=rng) * (xmax - xmin) + xmin\n",
    "observations_y = (torch.sin(observations_x * 2 * np.pi) + \n",
    "                  torch.randn(size=observations_x.size(), generator=rng) * noise_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f842a37a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Create the objective function (200 samples)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd119cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 200\n",
    "objective_x = torch.linspace(xmin, xmax, samples).reshape(-1, 1)\n",
    "objective_y = torch.sin(objective_x * 2 * np.pi)\n",
    "test_X = torch.linspace(xmin, xmax, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12804c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(observations_x, observations_y, \"*\", markersize=12, color='black', label=\"Observations (data)\")\n",
    "plt.plot(objective_x, objective_y, color='orange', label=\"Objective function (unknown)\")\n",
    "\n",
    "plt.xlabel(\"X feature\")\n",
    "plt.ylabel(\"Y target\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5c9524",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Build your first Gaussian process model</h2>\n",
    "\n",
    "\n",
    "For the covariance function (kernel), we choose a scaled radial basis function (RBF) kernel\n",
    "$$K_\\text{Scaled-RBF} (\\mathbf{x_1},\\mathbf{x_2})= \\theta_\\text{scale} \\exp\\left( - \\frac{1}{2}(\\mathbf{x_1} - \\mathbf{x_2})^\\top \\Sigma^{-2} (\\mathbf{x_1} - \\mathbf{x_2}) \\right),$$\n",
    "\n",
    "where $\\theta_\\text{scale}$ is the _outputscale_ parameter, $\\Sigma^{2}$ is the covariance matrix; In simple case, the _lengthscales_ for each input dimension are just the diagonal terms.\n",
    "\n",
    "c.f. [GPyTorch documentation](https://docs.gpytorch.ai/en/v1.6.0/kernels.html) for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468983b4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<h3>Define the desired kernel, in this case RBF:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25ee503",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "kernel = ScaleKernel(RBFKernel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8e2770",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<h3>Build your GP model with previous observations and selected kernel:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f7234e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = SingleTaskGP(train_X=observations_x, train_Y=observations_y, covar_module=kernel)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf21eaa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Hyperparameters</h3>\n",
    "\n",
    "Change the hyperparameters below to the following:\n",
    "- __lenghtscale__: 0.5\n",
    "- __signal variance__: 0.5\n",
    "- __model noise__: 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe92a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change the GP hyperparameters here\n",
    "model.covar_module.base_kernel.lengthscale = # fill here!\n",
    "model.covar_module.outputscale = # fill here!  # signal variance\n",
    "model.likelihood.noise_covar.noise = # fill here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87302f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Visualization of the prior of the GP model</h3>\n",
    "The GP just has a constant mean with some uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426079f5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the prior\n",
    "ax = sample_gp_prior_plot(model, test_X)\n",
    "ax.set_ylim(-2.5, 2.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8508c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 style=\"color:#e6541a;\">How do the hyperparameter values affect the drawn samples from the model?</h3>\n",
    "<p style=\"color:#e6541a;\">$\\implies$  Change the GP hyperparameters that you set before, in the previous cells.\n",
    "<p style=\"color:#e6541a;\">$\\implies$  How do you predict that the lengthscale, signal variance, and model noise will affect the shape of the samples? Test your theories</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7cf0fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Visualization of the posterior of the GP model</h3>\n",
    "\n",
    "- With the observations we generated previously we can build the posterior distribution\n",
    "- Observe the current status of our statistical model compared to our objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71c33f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change the GP hyperparameters here again\n",
    "model.covar_module.base_kernel.lengthscale = 0.5\n",
    "model.covar_module.outputscale = 0.5  # signal variance\n",
    "model.likelihood.noise_covar.noise = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910e9471",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the posterior\n",
    "sample_gp_posterior_plot(model, test_X, y_lim=(-2,2), n_samples=0, show_true_f=True, \n",
    "                         true_f_x= objective_x, true_f_y=objective_y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f67c104",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 style=\"color:#e6541a;\">Change the number of samples visualized</h3>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ Do this by changing the value of the <code>n_samples</code> argument in the <code>sample_gp_posterior_plot</code> function in the cell above\n",
    "<p style=\"color:#e6541a;\">$\\implies$ Can you understand better how the posterior is built?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2463c655",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 style=\"color:#e6541a;\">Change the hyperparameters again to fit the data by hand (two cells above)</h3>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ Avoid under- or overfitting\n",
    "<p style=\"color:#e6541a;\">$\\implies$ The GP mean should fit the data points as good as possible</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d74bf1",
   "metadata": {},
   "source": [
    "We will save the hyperparameters you found for comparison later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15622da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_lenghtscale = float(model.covar_module.base_kernel.lengthscale)\n",
    "manual_signal_variance = float(model.covar_module.outputscale)\n",
    "manual_model_noise = float(model.likelihood.noise_covar.noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb89c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Guide your hyperparameter setting!</h2>\n",
    "\n",
    "- One can use __prior knowledge__ (experience, archive data, ...) to constrain or even fix the _Gaussian process_ hyperparameters.\n",
    "- Another approach is to dynamically __adapt / fit__ the hyperparameters to the data.\n",
    "\n",
    "In BoTorch there is a convenient helper function [`fit_gpytorch_mll`](https://botorch.org/api/fit.html#botorch.fit) to fit the hyperparameters of the _Gaussian process_ model to the data using _marginal log-likelihood_ fits.\n",
    "\n",
    "- In this approach, the hyperparameters are varied until the likelihood is maximized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12001e5f",
   "metadata": {},
   "source": [
    "In the cell below, the `ExactMarginalLogLikelihood` function takes a likelihood object as argument, which is an attribute of the model we defined before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed9b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "mll = ExactMarginalLogLikelihood(model.likelihood, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c58140",
   "metadata": {},
   "source": [
    "Now let's perform the fit of the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c7505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_gpytorch_mll(mll);  # carries out the fit\n",
    "fit_gpytorch_model(mll);  # obsolete with new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634850b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show the results\n",
    "sample_gp_posterior_plot(model, test_X, y_lim=(-2,2), n_samples=0, \n",
    "                         show_true_f=True, true_f_x= objective_x, true_f_y=objective_y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa505819",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<h3 style=\"color:#e6541a;\">Now the GP model nicely fits the data!</h3>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ What causes that the model is not completely following the true objective function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3880975b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 style=\"color:#e6541a;\">Let's compare manual and automatically fitted hyperparameters</h3>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ Execute the cell below</p>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ Are the values very different?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07a081",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('Manual hyperparameters')\n",
    "print('- Lengthscale: ', manual_lenghtscale)\n",
    "print('- Signal variance: ', manual_signal_variance)\n",
    "print('- Model noise: ', manual_model_noise)\n",
    "\n",
    "print('')\n",
    "print('Fitted hyperparameters')\n",
    "print('- Lengthscale: ', np.round(float(model.covar_module.base_kernel.lengthscale), 2))\n",
    "print('- Signal variance: ', np.round(float(model.covar_module.outputscale), 2))\n",
    "print('- Model noise: ', np.round(float(model.likelihood.noise_covar.noise), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc03ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Build an acquisition function</h2>\n",
    "\n",
    "In Bayesian optimization, one uses an acquisition function to measure how interesting it would be to sample the function $f$ at a point $x$. \n",
    "\n",
    "The acquisition function $\\alpha$ is built based on the GP posterior, e.g. a probablistic surrogate model of the underlying function.\n",
    "\n",
    "BoTorch has implemented a variety of common acquisition functions, see [documentation](https://botorch.org/api/acquisition.html). \n",
    "\n",
    "In this tutorial we will use the __Upper Confidence Bound (UCB)__ function.\n",
    "\n",
    "$$ \\alpha_\\text{UCB} = \\mu (x) + \\sqrt{\\beta} \\sigma(x),$$\n",
    "\n",
    "where $\\mu$ and $\\sigma$ are the GP posterior mean and standard deviation respectively. \n",
    "\n",
    "<p style=\"color:#e6541a;\">$\\implies$ $\\beta$ is an hyperparameter controlling the <b>exploration-exploitation trade-off</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b42527",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 style=\"color:#e6541a;\">Explore how the UCB acquisition function behaves</h3>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ Change the <code>beta</code> argument of the <code>UpperConfidenceBound</code> function in the cell below</p>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ What does a larger beta yield?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c9e62c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "acq_UCB = UpperConfidenceBound(model, beta=4)\n",
    "plot_acq_with_gp(model, observations_x, observations_y, acq_UCB, test_X, show_true_f=True, \n",
    "                 true_f_x= objective_x, true_f_y=objective_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8becb24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 style=\"color:#e6541a;\">Try other acquisition functions</h3>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ Try expected improvement and probability of improvement by executing the cells below</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337edad1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "acq_EI = ExpectedImprovement(model,best_f=float(model.train_targets.max()))\n",
    "plot_acq_with_gp(model, observations_x, observations_y, acq_EI, test_X, show_true_f=True, \n",
    "                 true_f_x= objective_x, true_f_y=objective_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb09c56",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "acq_PI = ProbabilityOfImprovement(model, best_f=float(model.train_targets.max()))\n",
    "plot_acq_with_gp(model, observations_x, observations_y, acq_PI, test_X, show_true_f=True, \n",
    "                 true_f_x= objective_x, true_f_y=objective_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248dcedd",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<h3 style=\"color:#e6541a;\">What are the different sampling strategies of different acquisition functions?</h3>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ Which one is closer to finding the global maximum?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d555b09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Bonus: Bayesian exploration</h2>\n",
    "\n",
    "- Instead of doing optimization, we can tweak the acquisition function to only learn about the objective function\n",
    "- For example, instead of using $\\mu(x) + \\sqrt{\\beta}\\sigma(x)$ as in UCB, we can choose the acquisition to be $\\alpha(x)= \\sigma(x)$. \n",
    "    - In this way, we will only sample the function where the uncertainty is large.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2e95fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: center; width:100%;\">\n",
    "    <h2>Part III: Bayesian Optimization for Beam Positioning and Focusing in a Linac</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76bd054",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"text-align: center; vertical-align: middle;\">Beam positioning and focusing task at ARES Experimental Area (EA)</h2> \n",
    "\n",
    "We would like to focus and center the electron beam on a diagnostic screen using 2 corrector and 3 quadrupole magnets.\n",
    "\n",
    "![ARES EA Scheme](img/ares_magnets.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414dd26e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Look at the ARESEA environment</h2>\n",
    "\n",
    "We formulated the ARESEA task as a [OpenAI Gym](https://github.com/openai/gym) environment, which is a common approach for Reinforcement learning projects. This allows our algorithm to easily interface with both the simulation and real machine backends.\n",
    "\n",
    "In this part, you will get familiar with the environment for the beam focusing and positioning at ARES accelerator.\n",
    "\n",
    "First, let's create the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a943a0e7",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = ARESEA()\n",
    "\n",
    "# Wrap the environment with some utilities:\n",
    "env = RescaleAction(env, -1, 1)  # Normalize the action space to [-1,1]^n\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bea27e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 style=\"color:#e6541a;\">Get familiar with the Gym environment</h3>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ Change the magnet values, i.e. the actions</p>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ The actions are normalized to 1, so valid values are in the [0, 1] interval</p>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ The values of the <code>action</code> list in the cell below follows this magnet order: [Q1, Q2, CV, Q3, CH]</p>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ Observe the plot below, what beam does that magnet configuration yield? can you center and focus the beam by hand?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc1c18",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "action = [# fill here! ]\n",
    "action = np.array(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e0051f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Perform one step: update the env, observe new beam!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81cf695",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "observation, reward, done, info = env.step(action)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(16, 4)\n",
    "ax = plt.Axes(fig, [0.0, 0.0, 1.0, 1.0])\n",
    "ax.set_axis_off()\n",
    "fig.add_axes(ax)\n",
    "img = env.render(mode=\"rgb_array\")\n",
    "\n",
    "ax.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580d7e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 style=\"color:#e6541a;\">Take several steps in the environment</h3>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ Run the cell below, which will perform a linear scan of the values corresponding to the vertical corrector CV</p>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ What influence does CV have on the beam?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bbb691",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "steps = 10\n",
    "\n",
    "for i in range(steps):\n",
    "    env.step(np.array([0.2, -0.2, -.5 + 1 / steps * i, 0.3, 0]))\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    ax.imshow(img)\n",
    "    display(fig)\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb042312",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 style=\"color:#e6541a;\">Set a target beam you want to achieve</h3>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ Let's define the position $(\\mu_x, \\mu_y)$ and size $(\\sigma_x, \\sigma_y)$ of the beam on the screen</p>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ Modify the <code>target_beam</code> list below, where the order of the arguments is $[\\mu_x,\\sigma_x,\\mu_y,\\sigma_y]$</p>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ Take into account the dimensions of the screen ($\\pm$ 2e-3 m)</p>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ The target beam will be represented by a green circle on the screen</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e4c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_beam = [# fill here! ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f36330",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ARESEA(target_beam_mode=\"constant\",target_beam_values=target_beam, \n",
    "             magnet_init_mode=\"constant\", magnet_init_values=[15,-5,1e-3,5,2e-3])\n",
    "env = RescaleAction(env, -1, 1)  # Normalize the action space to [-1,1]^n\n",
    "observation, _ = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64584646",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Implementation of a full Bayesian optimization loop</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a9860",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def bayesian_optimize(env: gym.Env, last_observation, init_mode=\"current\", n_steps=50, \n",
    "                      acquisition=\"UCB\", beta=2, n_init=5, max_step_size:float=0.5, set_to_best=True, \n",
    "                      random_seed=None, show_plot=False, proximal=None,time_sleep=0.2):\n",
    "    ##################################################################\n",
    "    # Some preliminary settings\n",
    "    if random_seed is None:\n",
    "        random_seed = torch.random.seed() # random\n",
    "    rng = torch.random.manual_seed(random_seed)\n",
    "\n",
    "    # Initialization: some initial samples are needed to build a GP model\n",
    "\n",
    "    # First sample from the reset observation\n",
    "    initial_action = scale_action(env, last_observation)\n",
    "\n",
    "    if init_mode==\"current\":\n",
    "        X = torch.tensor([initial_action]).reshape(1,-1)\n",
    "        while len(X)<n_init:\n",
    "            last_action = X[0].detach().numpy()\n",
    "            bounds = get_new_bound(env, last_action, max_step_size)\n",
    "            new_action = np.random.uniform(low=bounds[0], high=bounds[1])\n",
    "            new_action_tensor = torch.tensor(new_action, dtype=torch.double).reshape(\n",
    "                1, -1\n",
    "            )\n",
    "            X = torch.cat([X,new_action_tensor])\n",
    "    else:  # sample purely randomly\n",
    "        X = torch.tensor([], dtype=torch.double)\n",
    "        while len(X)<n_init:\n",
    "            new_action = env.action_space.sample()\n",
    "            new_action_tensor = torch.tensor(new_action, dtype=torch.double).reshape(\n",
    "                1, -1\n",
    "            )\n",
    "            X = torch.cat([X,new_action_tensor])\n",
    "    \n",
    "    Y = torch.zeros(n_init,1,dtype=torch.double)\n",
    "    # sample initial points\n",
    "    for i,x in enumerate(X):  \n",
    "        _, reward, _, _ = env.step(x.numpy())\n",
    "        Y[i] = reward\n",
    "\n",
    "    if show_plot:\n",
    "        fig = plt.figure()\n",
    "        fig.set_size_inches(16,4)\n",
    "        ax_progress = plt.Axes(fig, [0.0, 0.0, 0.25, 1.0])\n",
    "        ax = plt.Axes(fig, [0.25, 0.0, 1.0, 1.0])\n",
    "        ax.set_axis_off()\n",
    "        fig.add_axes(ax_progress)\n",
    "        fig.add_axes(ax)\n",
    "        ax_progress.set_xlabel(\"Steps\")\n",
    "        ax_progress.set_ylabel(r\"log(MAE($b_\\mathrm{current}, b_\\mathrm{target}$))\")\n",
    "        ax_progress.set_title(f\"Best objective: {float(Y.max())}\")\n",
    "    \n",
    "    ##################################################################\n",
    "    # Actual BO logic\n",
    "    for i in range(n_steps):\n",
    "        # Fit GP model to the observed data\n",
    "        kernel=ScaleKernel(MaternKernel())\n",
    "        model = SingleTaskGP(X, Y,covar_module=kernel,outcome_transform=Standardize(m=1))\n",
    "        # model.likelihood.noise = 1e-2\n",
    "        # model.likelihood.noise_covar.raw_noise.requires_grad_(False)\n",
    "        mll = ExactMarginalLogLikelihood(model.likelihood,model)\n",
    "#         fit_gpytorch_mll(mll)\n",
    "        fit_gpytorch_model(mll)\n",
    "        \n",
    "        # Build acquisition\n",
    "        if acquisition==\"UCB\":\n",
    "            acq = UpperConfidenceBound(model, beta=beta)\n",
    "        elif acquisition==\"EI\":\n",
    "            ymax = float(Y.max())\n",
    "            acq = ExpectedImprovement(model, best_f=ymax)\n",
    "        \n",
    "        if proximal is not None:\n",
    "            acq = ProximalAcquisitionFunction(acq,proximal_weights=proximal)\n",
    "\n",
    "        # Choose next action\n",
    "        new_bound = get_new_bound(env,X[-1].detach().numpy(),max_step_size)\n",
    "        x_next, _ = optimize_acqf(acq, bounds=torch.tensor(new_bound),q=1,num_restarts=16,raw_samples=256,options={\"maxiter\": 200},) \n",
    "        # Apply the action\n",
    "        observation, reward, done, _ = env.step(x_next.numpy().flatten())\n",
    "        \n",
    "        # Append data (with correct shape)\n",
    "        Y = torch.cat([Y,torch.tensor([[reward]])])\n",
    "        X = torch.cat([X,torch.tensor(x_next).reshape(1,-1)])\n",
    "        \n",
    "\n",
    "        # Plotting\n",
    "        if show_plot:\n",
    "            img = env.render(mode=\"rgb_array\")\n",
    "            ax.imshow(img)\n",
    "            ax_progress.clear()\n",
    "            ax_progress.plot(Y.detach().numpy().flatten())\n",
    "            ax_progress.set_title(f\"Best objective: {float(Y.max()):.2f}\")\n",
    "            ax_progress.set_xlabel(\"Steps\")\n",
    "            ax_progress.set_ylabel(r\"log(MAE($b_\\mathrm{current}, b_\\mathrm{target}$))\")\n",
    "            display(fig)\n",
    "            clear_output(wait=True)\n",
    "            time.sleep(time_sleep)\n",
    "\n",
    "        # Check termination\n",
    "        if done:\n",
    "            print(\"Target beam is reached\")\n",
    "            set_to_best=False  # no need to reset\n",
    "            break\n",
    "    # Set to best observed if not reaching target in the allowed steps\n",
    "    if set_to_best:\n",
    "        x_best = X[Y.flatten().argmax()].numpy()\n",
    "        env.step(x_best)\n",
    "        # Plotting\n",
    "        if show_plot:\n",
    "#             print(f\"Best objective: {float(Y.max())}\")\n",
    "            img = env.render(mode=\"rgb_array\")\n",
    "            ax.imshow(img)\n",
    "            display(fig)\n",
    "            clear_output(wait=True)\n",
    "            time.sleep(time_sleep)\n",
    "\n",
    "    # Return some information\n",
    "    opt_info = {\n",
    "        \"X\": X,\n",
    "        \"Y\": Y,\n",
    "        \"best\": Y.max(),\n",
    "    }\n",
    "    return opt_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadcffea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Let's apply Bayesian optimization to this problem</h2>\n",
    "\n",
    "- We will use the loop implemented in the cell above\n",
    "- In order to quantify how the algorithm is performing, we will use the __log maximum aboslute error (L1 error)__ as metric:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "f(x) &= -\\log(\\mathrm{MAE}(b_\\mathrm{current},b_\\mathrm{target})) \\\\\n",
    "&= - \\log \\sum_i |b_{\\mathrm{current},i}-b_{\\mathrm{target},i}| \n",
    "\\end{aligned}$$\n",
    "\n",
    "- We do this because using just the difference between the target and the real beam is very small (sub mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b743b55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 style=\"color:#e6541a;\">Running Bayesian optimization</h3>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ Run the cell below and observe the optimization. Did it achieve the target? How did the performance metric evolve during the optimization?</p>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ Change the <code>max_step_size</code> argument. This indicates how much each action changes per step. What do you observe if the steps are too large?</p>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ Do you think that adding more iteration steps will help?</p>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ Can you think of other ways of solving this focusing and steering problem automatically?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abe356c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "env = ARESEA(target_beam_mode=\"constant\",target_beam_values=target_beam, magnet_init_mode=\"constant\", \n",
    "             magnet_init_values=[15, -5, 1e-3, 5, 2e-3])\n",
    "env = RescaleAction(env, -1, 1)  # Normalize the action space to [-1,1]^n\n",
    "observation, _ = env.reset()\n",
    "\n",
    "opt_info = bayesian_optimize(env, last_observation=observation, n_steps=50, max_step_size=0.1,  \n",
    "                             show_plot=True, beta=0.2, time_sleep=0.1)\n",
    "\n",
    "# Another advanced technique \"Proximal biasing\" uses soft step size limit\n",
    "# opt_info = bayesian_optimize(env, last_observation=observation,n_steps=50,max_step_size=1,  \n",
    "# show_plot=True, beta=0.2, proximal=torch.ones(5)*0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b805c32d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 style=\"color:#e6541a;\">Exploration and exploitation with acquisition functions</h3>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ Change the <code>beta</code> argument of the <code>bayesian_optimize</code> function in the cell below to 2. What is different in this optimization compared to the previous one? How does the performance metric evolve?</p>\n",
    "<p style=\"color:#e6541a;\">$\\implies$ Use a different acquisition function, like expected improvement \"EI\"</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556efc28",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "env = ARESEA(target_beam_mode=\"constant\",target_beam_values=target_beam, magnet_init_mode=\"constant\", \n",
    "             magnet_init_values=[15, -5, 1e-3, 5, 2e-3])\n",
    "env = RescaleAction(env, -1, 1)  # Normalize the action space to [-1,1]^n\n",
    "observation, _ = env.reset()\n",
    "\n",
    "\n",
    "### Change the value of beta, see how it impacts the optimization process; \n",
    "### Or switch to another acquisition\n",
    "beta = 2.0\n",
    "acquisition = \"UCB\"\n",
    "\n",
    "opt_info = bayesian_optimize(env, observation, n_steps=40, acquisition=acquisition, beta=beta,\n",
    "                             max_step_size=0.3, show_plot=True, time_sleep=0.05)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122975e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "- Bayesian Optimization is a numerical _global_ optimization method for black-box functions.\n",
    "\n",
    "- BO uses a _Gaussian process_ as a statistical surrogate model of the objective.\n",
    "  - GP is characterized by its mean and covariance (kernel) function.\n",
    "  - Hyperparamters can be dynamically fitted to the data.\n",
    "  - Prior knowledge can be included for setting the priors.\n",
    "- BO uses the _acquisition function_ to guide the optimziation\n",
    "  - One should choose an acquisition suitable for the task.\n",
    "  - Hyperparameters of the acquisition function also affects the optimization behaviour.\n",
    "- BO is an __optimization__ method. It is not designed for a _control_ task.\n",
    "- It is adequate for a moderate amount of dimensions (~100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640afa6d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Literature\n",
    "\n",
    "Hopefully you have learned something about BO, if you want to try it yourself afterwards, below are some interesting resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc640ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Publication: Various applications of BO in accelerator physics\n",
    "\n",
    "- KARA, KIT: [Bayesian Optimization of the Beam Injection Process into a Storage Ring](https://arxiv.org/abs/2211.09504) Beam injection optimization using BO.\n",
    "- LCLS, SLAC: [Bayesian optimization of FEL performance at LCLS](https://accelconf.web.cern.ch/ipac2016/doi/JACoW-IPAC2016-WEPOW055.html), [Bayesian Optimization of a Free-Electron Laser](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.124.124801) FEL performance tuning with quadrupoles\n",
    "- LUX, DESY: [Bayesian Optimization of a Laser-Plasma Accelerator](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.126.104801) LPA tuning to improve bunch quality with laser energy, focus position, and gas flows.\n",
    "- Central Laser Facility, Rutherfold UK: [Automation and control of laser wakefield accelerators using Bayesian optimization](https://www.nature.com/articles/s41467-020-20245-6), LWFA performance tuning with different objective functions\n",
    "- PSI, SwissFEL: [Tuning particle accelerators with safety constraints using Bayesian optimization](https://doi.org/10.1103/PhysRevAccelBeams.25.062802) [Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional Subspaces](https://arxiv.org/abs/1902.03229) : BO with safety contraints to protect the machine\n",
    "- SLAC, ANL: [Multiobjective Bayesian optimization for online accelerator tuning](https://journals.aps.org/prab/abstract/10.1103/PhysRevAccelBeams.24.062801), multiobjective optimization for accelerator tuning,[Turn-key constrained parameter space exploration for particle accelerators using Bayesian active learning](https://www.nature.com/articles/s41467-021-25757-3), Bayesian active learning for effcient exploration of the parameter space. [Differentiable Preisach Modeling for Characterization and Optimization of Particle Accelerator Systems with Hysteresis](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.128.204801) hysteris modelling with GP, and application of hysteresis-aware BO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea38d8a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Books and papers on Bayesian optimization in general\n",
    "\n",
    "- C. E. Rasmussen and C. K.I. Williams, [Gaussian Processes for Machine Learning](https://gaussianprocess.org/gpml/): __the__ classic textbook of Gaussian process.\n",
    "- Eric Brochu, [A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning](https://arxiv.org/abs/1012.2599): a comprehensive tutorial on Bayesian optimization with some application cases at that time (2010).\n",
    "- Peter I. Frazier, [A Tutorial on Bayesian Optimization](https://arxiv.org/abs/1807.02811): a more recent (2018) tutorial paper covering the most important aspects of BO, and some advanced variants of BO (parallel, multi-fidelity, multi-task)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eca4148",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Optimization / Gaussian Process packages in python\n",
    "\n",
    "Below is a incomplete selection of python packages for BO and GP, each with its own strength and drawback.\n",
    "\n",
    "- [scikit-learn Gaussian processes](https://scikit-learn.org/stable/modules/gaussian_process.html#) : recommended for sklearn users, not as powerful as other packages.\n",
    "- [GPyTorch](https://gpytorch.ai/) : a rather new package implemented natively in PyTorch, which makes it very performant. Also comes with a Bayesian optimization package [BOTorch](https://botorch.org/), offering a variety of different optimization methods (mult-objective, parallelization...). Both packages are being actively developed maintained; __recommended state-of-the-art tool for BO practitioners__.\n",
    "- [GPflow](https://www.gpflow.org/) : a GP package implemented in TensorFlow, it also has a large community and is being actively maintained; The new BO package [Trieste](https://secondmind-labs.github.io/trieste) is built on it.\n",
    "- [GPy](http://sheffieldml.github.io/GPy/) from the Sheffield ML group : A common/classic choice for building GP model, includes a lot of advanced GP variants; However in recent years it is not so actively maintained. It comes with the accompanying Bayesian optimization package [GPyOpt](https://github.com/SheffieldML/GPyOpt), for which the maintainance stoped since 2020.\n",
    "- [Dragonfly] : a open-source BO package; offers also command line tool, easy to use if you are a practitioner. However if one has less freedom to adapt and expand the code.\n",
    "\n",
    "C.f. the [wikipedia page](https://en.wikipedia.org/wiki/Comparison_of_Gaussian_process_software#Comparison_table) for a more inclusive table with GP packages for other languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe810e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other Resources\n",
    "\n",
    "- Another similarly structured BO tutorial using the _scikit-learn_ GP package, given at the [2022 MT ARD ST3 ML Workshop](https://github.com/ansantam/2022-MT-ARD-ST3-ML-workshop)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "rise": {
   "enable_chalkboard": true,
   "footer": "<p>Fachbereich Elektrotechnik und Informationstechnik (etit)   |   Institut für Teilchenbeschleunigung und Elektromagnetische Felder (TEMF)   |   Dr. Andrea Santamaria Garcia & Chenran Xu</p>",
   "header": "<img src='https://upload.wikimedia.org/wikipedia/de/thumb/2/24/TU_Darmstadt_Logo.svg/640px-TU_Darmstadt_Logo.svg.png' />",
   "scroll": true,
   "theme": "simple",
   "transition": "none"
  },
  "vscode": {
   "interpreter": {
    "hash": "aefbd9db9e80b76479690376e0f6858b117f02d7b00937331627c3692732c1c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
